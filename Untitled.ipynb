{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5463974a-3cb5-40a7-8a1a-6c5f09df4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "487734ec-bc27-4acd-a8f6-c813e9467045",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b38f5f81-dec3-4b80-8047-d3b29bd8d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstnamexxxx\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "797a2c48-e1fc-4162-a651-407d32768bfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', '', 'Smith', '36636', 'M', 3000),\n",
       " ('Michael', 'Rose', '', '40288', 'M', 4000),\n",
       " ('Robert', '', 'Williams', '42114', 'M', 4000),\n",
       " ('Maria', 'Anne', 'Jones', '39192', 'F', 4000),\n",
       " ('Jen', 'Mary', 'Brown', '', 'F', -1)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b4daeff-5334-4b16-9fe1-1237defd8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstnamexxxx: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f52a75c-3746-454d-a911-94669610b8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE data.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c675ca8-c82a-4ed1-8bb6-201ea62fe908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"db.test4\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bd53dc80-3f38-45da-aad7-7b67d8f47b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = spark.table(\"db.test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b38ec203-ed4d-419b-840c-9b97239c57f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+-----+------+------+\n",
      "|firstnamexxxx|middlename|lastname|   id|gender|salary|\n",
      "+-------------+----------+--------+-----+------+------+\n",
      "|        James|          |   Smith|36636|     M|  3000|\n",
      "|      Michael|      Rose|        |40288|     M|  4000|\n",
      "|       Robert|          |Williams|42114|     M|  4000|\n",
      "|        Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|          Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+-------------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3525109-b3ed-4b99-a9ad-1d258822f2a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkConnectGrpcException",
     "evalue": "(org.apache.iceberg.exceptions.CommitStateUnknownException) file:/warehouse/db.db/sampleTable3 is not a directory or unable to create one\nCannot determine whether the commit was successful or not, the underlying data files may or may not be needed. Manual intervention via the Remove Orphan Files Action can remove these files when a connection to the Catalog can be re-established if the commit was actually unsuccessful.\nPlease check to see whether or not your commit was successful before retrying this commit. Retrying an already successful operation will result in duplicate records or unintentional modifications.\nAt this time no files will be deleted including possibly unused manifest lists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE db.sampleTable3 (number Int, word String)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/session.py:550\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery: \u001b[38;5;28mstr\u001b[39m, args: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], List]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    549\u001b[0m     cmd \u001b[38;5;241m=\u001b[39m SQL(sqlQuery, args)\n\u001b[0;32m--> 550\u001b[0m     data, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame\u001b[38;5;241m.\u001b[39mwithPlan(CachedRelation(properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/client/core.py:982\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    980\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m    981\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m--> 982\u001b[0m data, _, _, _, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (data\u001b[38;5;241m.\u001b[39mto_pandas(), properties)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/client/core.py:1283\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, self_destruct)\u001b[0m\n\u001b[1;32m   1280\u001b[0m schema: Optional[StructType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(req):\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1285\u001b[0m         schema \u001b[38;5;241m=\u001b[39m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/client/core.py:1264\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1262\u001b[0m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/client/core.py:1503\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;124;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 1503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/IVenv3.9.16/lib/python3.9/site-packages/pyspark/sql/connect/client/core.py:1539\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             info \u001b[38;5;241m=\u001b[39m error_details_pb2\u001b[38;5;241m.\u001b[39mErrorInfo()\n\u001b[1;32m   1538\u001b[0m             d\u001b[38;5;241m.\u001b[39mUnpack(info)\n\u001b[0;32m-> 1539\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (org.apache.iceberg.exceptions.CommitStateUnknownException) file:/warehouse/db.db/sampleTable3 is not a directory or unable to create one\nCannot determine whether the commit was successful or not, the underlying data files may or may not be needed. Manual intervention via the Remove Orphan Files Action can remove these files when a connection to the Catalog can be re-established if the commit was actually unsuccessful.\nPlease check to see whether or not your commit was successful before retrying this commit. Retrying an already successful operation will result in duplicate records or unintentional modifications.\nAt this time no files will be deleted including possibly unused manifest lists."
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE db.sampleTable3 (number Int, word String)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7d435-7292-4e8c-82c6-12c30ce46e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
